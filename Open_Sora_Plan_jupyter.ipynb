{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shlok71/SkyblockCreation-Datapack/blob/main/Open_Sora_Plan_jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "1cee335e-f7a6-4a8f-fb7d-8d465e33782f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Open-Sora-Plan-v1.0.0-hf'...\n",
            "remote: Enumerating objects: 373, done.\u001b[K\n",
            "remote: Counting objects: 100% (373/373), done.\u001b[K\n",
            "remote: Compressing objects: 100% (268/268), done.\u001b[K\n",
            "remote: Total 373 (delta 93), reused 373 (delta 93), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (373/373), 441.57 KiB | 17.66 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n",
            "Downloading opensora/eval/fvd/styleganv/i3d_torchscript.pt (51 MB)\n",
            "Error downloading object: opensora/eval/fvd/styleganv/i3d_torchscript.pt (bec6519): Smudge error: Error downloading opensora/eval/fvd/styleganv/i3d_torchscript.pt (bec6519f66ea534e953026b4ae2c65553c17bf105611c746d904657e5860a5e2): [bec6519f66ea534e953026b4ae2c65553c17bf105611c746d904657e5860a5e2] Object does not exist on the server: [404] Object does not exist on the server\n",
            "\n",
            "Errors logged to /content/Open-Sora-Plan-v1.0.0-hf/.git/lfs/logs/20250618T214826.06953234.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: opensora/eval/fvd/styleganv/i3d_torchscript.pt: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "/content/Open-Sora-Plan-v1.0.0-hf\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.6/916.6 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/Open-Sora-Plan-v1.0.0-hf\n",
        "%cd /content/Open-Sora-Plan-v1.0.0-hf\n",
        "!pip install -q diffusers==0.24.0 gradio==3.50.2 einops==0.7.0 omegaconf==2.1.1 pytorch-lightning==1.4.2 torchmetrics==0.6.0 torchtext==0.6 accelerate==0.28.0\n",
        "\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "import torch\n",
        "from diffusers import PNDMScheduler\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from gradio.components import Textbox, Video, Image\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "\n",
        "from opensora.models.ae import ae_stride_config, getae, getae_wrapper\n",
        "from opensora.models.diffusion.latte.modeling_latte import LatteT2V\n",
        "from opensora.sample.pipeline_videogen import VideoGenPipeline\n",
        "from opensora.serve.gradio_utils import block_css, title_markdown, randomize_seed_fn, set_env, examples, DESCRIPTION\n",
        "\n",
        "def generate_img(prompt, sample_steps, scale, seed=0, randomize_seed=False, force_images=False):\n",
        "    seed = int(randomize_seed_fn(seed, randomize_seed))\n",
        "    set_env(seed)\n",
        "    video_length = transformer_model.config.video_length if not force_images else 1\n",
        "    height, width = int(args.version.split('x')[1]), int(args.version.split('x')[2])\n",
        "    num_frames = 1 if video_length == 1 else int(args.version.split('x')[0])\n",
        "    with torch.no_grad():\n",
        "        videos = videogen_pipeline(prompt,\n",
        "                                   video_length=video_length,\n",
        "                                   height=height,\n",
        "                                   width=width,\n",
        "                                   num_inference_steps=sample_steps,\n",
        "                                   guidance_scale=scale,\n",
        "                                   enable_temporal_attentions=not force_images,\n",
        "                                   num_images_per_prompt=1,\n",
        "                                   mask_feature=True,\n",
        "                                   ).video\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    videos = videos[0]\n",
        "    tmp_save_path = 'tmp.mp4'\n",
        "    imageio.mimwrite(tmp_save_path, videos, fps=24, quality=9)  # highest quality is 10, lowest is 0\n",
        "    display_model_info = f\"Video size: {num_frames}×{height}×{width}, \\nSampling Step: {sample_steps}, \\nGuidance Scale: {scale}\"\n",
        "    return tmp_save_path, prompt, display_model_info, seed\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = type('args', (), {\n",
        "        'ae': 'CausalVAEModel_4x8x8',\n",
        "        'force_images': False,\n",
        "        'model_path': 'LanguageBind/Open-Sora-Plan-v1.0.0',\n",
        "        'text_encoder_name': 'DeepFloyd/t5-v1_1-xxl',\n",
        "        'version': '65x512x512'\n",
        "    })\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Load model:\n",
        "    transformer_model = LatteT2V.from_pretrained(args.model_path, subfolder=args.version, torch_dtype=torch.float16, cache_dir='cache_dir').to(device)\n",
        "\n",
        "    vae = getae_wrapper(args.ae)(args.model_path, subfolder=\"vae\", cache_dir='cache_dir').to(device, dtype=torch.float16)\n",
        "    vae.vae.enable_tiling()\n",
        "    image_size = int(args.version.split('x')[1])\n",
        "    latent_size = (image_size // ae_stride_config[args.ae][1], image_size // ae_stride_config[args.ae][2])\n",
        "    vae.latent_size = latent_size\n",
        "    transformer_model.force_images = args.force_images\n",
        "    tokenizer = T5Tokenizer.from_pretrained(args.text_encoder_name, cache_dir=\"cache_dir\")\n",
        "    text_encoder = T5EncoderModel.from_pretrained(args.text_encoder_name, cache_dir=\"cache_dir\",\n",
        "                                                  torch_dtype=torch.float16).to(device)\n",
        "\n",
        "    # set eval mode\n",
        "    transformer_model.eval()\n",
        "    vae.eval()\n",
        "    text_encoder.eval()\n",
        "    scheduler = PNDMScheduler()\n",
        "    videogen_pipeline = VideoGenPipeline(vae=vae,\n",
        "                                         text_encoder=text_encoder,\n",
        "                                         tokenizer=tokenizer,\n",
        "                                         scheduler=scheduler,\n",
        "                                         transformer=transformer_model).to(device=device)\n",
        "\n",
        "\n",
        "    demo = gr.Interface(\n",
        "        fn=generate_img,\n",
        "        inputs=[Textbox(label=\"\",\n",
        "                        placeholder=\"Please enter your prompt. \\n\"),\n",
        "                gr.Slider(\n",
        "                    label='Sample Steps',\n",
        "                    minimum=1,\n",
        "                    maximum=500,\n",
        "                    value=50,\n",
        "                    step=10\n",
        "                ),\n",
        "                gr.Slider(\n",
        "                    label='Guidance Scale',\n",
        "                    minimum=0.1,\n",
        "                    maximum=30.0,\n",
        "                    value=10.0,\n",
        "                    step=0.1\n",
        "                ),\n",
        "                gr.Slider(\n",
        "                    label=\"Seed\",\n",
        "                    minimum=0,\n",
        "                    maximum=203279,\n",
        "                    step=1,\n",
        "                    value=0,\n",
        "                ),\n",
        "                gr.Checkbox(label=\"Randomize seed\", value=True),\n",
        "                gr.Checkbox(label=\"Generate image (1 frame video)\", value=False),\n",
        "                ],\n",
        "        outputs=[Video(label=\"Vid\", width=512, height=512),\n",
        "                 Textbox(label=\"input prompt\"),\n",
        "                 Textbox(label=\"model info\"),\n",
        "                 gr.Slider(label='seed')],\n",
        "        title=title_markdown, description=DESCRIPTION, theme=gr.themes.Default(), css=block_css,\n",
        "        examples=examples,\n",
        "    )\n",
        "    demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}